# -*- coding: utf-8 -*-
"""Lesson4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IQFyfSwqeLzVH4M8VXgUMsbdhvnRetT-
"""

import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


print(torch.__version__)

"""## Introduction to Tensors"""

scalar= torch.tensor(7)

scalar

scalar.ndim

"""only for scalar does item work"""

scalar.item()

vector = torch.tensor([7,6])

vector

"""how many square bracket is the dimension of tensor"""

vector.ndim

vector.shape

vector[1]

#TENSORS
#this is shape
TENSOR = torch.tensor([[[1,2],[4,5],[7,8]]])
#torch.Size([1, 3, 2])
#[[[1,2],[4,5],[7,8]]]<-----this bracket has 1 element
#[[1,2],[4,5],[7,8]]<----- this bracket has 3 elements
#all the elements inside the last bracket has 2 elements

TENSOR

TENSOR.shape

TENSOR.ndim

"""RANDOM TENSORS

"""

random_tensor= torch.rand(1,3,2)
 random_tensor

random_image_size_tensor= torch.rand(size=(3,224,224))
random_image_size_tensor.shape,random_image_size_tensor.ndim

"""Tensors of 0 and 1s"""

zero = torch.zeros(size=(3,4))
zero

ones = torch.ones(3,4)

ones

ones.dtype

"""Creating a range of tensore and tensors like"""

torch.arange(0,10)

range = torch.arange(0,100,10)

range

#tensor like
ten_zeroes = torch.zeros_like(range)

ten_zeroes

"""Tensor data types


Note: Tensor datatypes are one of 3 big issues we run into in pytorch and data science

1.Tensor not right datatype

2.Tensor not right shape

3.Tensor not on the right device
"""

float_tensor = torch.tensor([3,6,9],
                            dtype=None,#data type in tensor
                            device=None,#device tensor is on, diffrent tensor in different device cause issue
                            requires_grad=False)#whether or not to track gradients with this tensors operation

float

float_tensor

float_16_tensor = float_tensor.type(torch.float16)

float_16_tensor

float_16_tensor*float_tensor

"""Lesson 2"""

int_32_tensor = torch.tensor([3,6,9],dtype=torch.int32)

int_32_tensor

float_tensor*int_32_tensor

"""Getting information from tensors

1.Tensor not right datatype- to get datatype from a tensor, use tensor.dtype

2.Tensor not right shape- to get shape, use tensor.shape

3.Tensor not on the right device- to get device from a tensor, use tensor.device
"""

some_tensor = torch.rand(3,4)

some_tensor

"""Finding out details about the tensors"""

some_tensor.dtype

some_tensor.shape

some_tensor.device

"""Manipulation Tensors (Tensor operations)

Tensor operations include:

1. Addition

2. Subtraction

3. Multiplication (element wise)

4. Division

5. Matrix multiplication
"""

new_tensor= torch.tensor([1,2,3])
new_tensor+10

new_tensor*10

new_tensor-10

#in built multipy function
 torch.mul(new_tensor,10)

torch.add(new_tensor,10)

"""2 main ways of performing multiplication in neural network and deep learning

1. Element wise multiplication

2. Matrix multiplication

There are two main rules that performing matrix mutliplication needs to satisfy:

1. The inner dimensions must match

  * (3,2) @ (2,3) wont work

  * (2,3) @ (3,2) will work

  * (3,2) @ (2,3) will work

2. The resulting matrix have the shape of the outer matrix




"""

#Element wise multiplication
tensor = torch.tensor([2,3,4])
tensor

element_wise_multiplicaton = tensor*tensor
element_wise_multiplicaton

matrix_wise_multiplication = torch.matmul(tensor,tensor)
matrix_wise_multiplication

new_matrix= torch.mm(torch.rand(2,3),torch.rand(3,4)) #mm is same as matmul

new_matrix

"""One of the most common errors in deep learning: Shape errors

To fix the tensor shape issues, we can manipulate the shape of one of our tensors using a transpose


"""

tensor_A = torch.rand(3,4)
tensor_A,tensor_A.shape

tensor_A.T,tensor_A.T.shape

"""LESSON 3

TENSOR AGGREGATION

Finding the min, max, mean, sum,etc
"""

x = torch.arange(0,100,9)
x

#Minimum
torch.min(x)

#Max
torch.max(x)

#sum
torch.sum(x),x.sum()

#mean

#input dtype should be either floating point or complex dtypes but not long
torch.mean(x.type(torch.float32)),(x.type(torch.float32)).mean()

"""Finding the position of min and max"""

# position of minimum value
x.argmin(),torch.argmin(x)

#position of maximum value
x.argmax(),torch.argmax(x)

"""LESSON 4

Reshaping,viewing, stacking, squeezing, unsqueezing and permuting tensors

Reshaping = reshapes an input tensor to a defined shape

View = Return a view of input tensor of certain shape but keep the same memory as the original tensor

Stacking = combine multiple tensors on top of each other(vstack) or side by side (hstack)

Squeeze= removes all `1` dimension from a *tensor*

Unsqueeze = add a `1` dimension to a target tensor

Permute = return a view of the input with dimensions permuted (swapped) in a certain way
"""

new_variable_x= torch.arange(1,13)
new_variable_x

#reshape helps to change the dimension of tensor form certain kind to certain kind

new_variable_x.reshape(2,6),new_variable_x.shape

#change the view
z= new_variable_x.view(3,4)
z,z.shape

#changing z changes new_variable_x because  a view of tensor shares the same memory as the original input
z[:,0]=5
z,new_variable_x

#Stack tensors on top of each other
x_stacked = torch.stack([new_variable_x,new_variable_x,new_variable_x,new_variable_x],dim=1)
x_stacked

#squeeze
y= torch.rand(1,1,3)
y,y.shape
sqz= y.squeeze()

sqz,sqz.shape
#all one dimensions removed

#unsqueeze- adds single dimension at specific dim
sqz.unsqueeze(0).shape,sqz.unsqueeze(1).shape
#at 0th index 1 is added,#at 1th index 1 is added

#torch.permute
#shares the same memory as the variable
rand_x = torch.rand(size=(224,224,3))

permuted_x = rand_x.permute(2,0,1)

print(f"Previous shape:{rand_x.shape}")
print(f"New shape:{permuted_x.shape}")

rand_x[0,0,0] =72818
rand_x[0,0,0],permuted_x[0,0,0]

"""Indexing(selecting data from tensors)   """

#Create a tensor
x=torch.arange(1,13).reshape(1,4,3)
x,x.shape

x[0][2]

x[0,2,2]

#getting all values of 0th and 1st dimension but only index 1 of 2nd dimension
x[:,:,1]

x[:,1,1]

x[0,:,2]

"""Pytorch and Numpy"""

#NumPy array to tensor
import torch
import numpy as np

array = np.arange(1.0,13.0)# data type is float32
tensor= torch.from_numpy(array)
array,tensor

#tensor to numpy
tensor = torch.ones(7)
numpy_tensor = tensor.numpy()
tensor, numpy_tensor

"""Reproducibility
-trying to takeout randomness out of random

we use random seed to reduce the randomness in neural networks
"""

random_tensor_A = torch.rand(3,4)
random_tensor_B = torch.rand(3,4)

random_tensor_A,random_tensor_B,random_tensor_A==random_tensor_B

RANDOM_SEED =10
torch.manual_seed(RANDOM_SEED)
random_tensor_c = torch.rand(3,4)

torch.manual_seed(RANDOM_SEED)
random_tensor_d = torch.rand(3,4)

random_tensor_c,random_tensor_d,random_tensor_c==random_tensor_d

"""Running tensors and Pytorch objects on the GPUs(and making faster computations)

GPU= faster thanks to CUDA hardware

Getting GPU:
1. Google Collab
2. Own GPU- buy GPU
3. Cloud computing
"""

#check for GPU access with Pytorch
torch.cuda.is_available()

#setup device agnostic code
device = "cuda" if torch.cuda.is_available() else "cpu"
device

torch.cuda.device_count()

"""Putting a tensor(and models) on the GPU"""

#default cpu
tensor= torch.tensor([1,2,3])
tensor,tensor.device

#using gpu
tensor_on_gpu = tensor.to(device)
tensor_on_gpu

tensor_on_cpu = tensor.to("cpu")

tensor_on_cpu.device

array = tensor_on_cpu.numpy()# array= tensor_on_gpu.cpu().numpy()

array